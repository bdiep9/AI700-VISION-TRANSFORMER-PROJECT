{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyY2nBtbw4C_"
      },
      "source": [
        "# Project Overview:\n",
        "This notebook implements a Vision Transformer (ViT) model for multi-class skin cancer classification using dermatoscopic images. The goal is to train a deep learning model capable of accurately classifying skin lesions into predefined diagnostic categories.\n",
        "\n",
        "The workflow includes:\n",
        "\n",
        "-Dataset loading and preprocessing\n",
        "\n",
        "-Data augmentation\n",
        "\n",
        "-Model initialization using a pretrained ViT\n",
        "\n",
        "-Training and validation\n",
        "\n",
        "-Model evaluation and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KCDEB1Kn8pC"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision timm scikit-learn albumentations pandas matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "QmhiwcaGoEWK",
        "outputId": "85812f8e-2fae-48dd-aa52-085f8daf5c5b"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()   # â¬…ï¸ choose your kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T4lU7OBpXNS",
        "outputId": "3e32b0ae-252e-4cfc-ddaa-eece4019370f"
      },
      "outputs": [],
      "source": [
        "!mv \"kaggle (17).json\" kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM4bi4yypYsR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HYyyfYvpaxQ",
        "outputId": "d7b1191a-91b4-421d-feef-00dfb883936c"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets list -s ham10000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NYzuaB_zoVN"
      },
      "source": [
        "# Connect to HAM10000 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDTkhjLmpapA",
        "outputId": "369b90fa-073c-4272-d833-aa595195eb59"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p /content --unzip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO_wz58jpgmz"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/HAM10000_images\n",
        "!cp /content/HAM10000_images_part_1/* /content/HAM10000_images/\n",
        "!cp /content/HAM10000_images_part_2/* /content/HAM10000_images/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnvymIJszwjV"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCkzxXb9rGiF",
        "outputId": "a241bfe2-61e4-41d6-f278-cdb12d53b414"
      },
      "outputs": [],
      "source": [
        "import torch, timm, numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import pandas as pd, os\n",
        "\n",
        "# ==== CONFIG ====\n",
        "CSV_FILE = \"/content/HAM10000_metadata.csv\"\n",
        "IMG_DIR = \"/content/HAM10000_images\"\n",
        "OUT_DIR = \"/content/outputs\"\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 3e-5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ==== LOAD DATA ====\n",
        "df = pd.read_csv(CSV_FILE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9kCG9BWz4He"
      },
      "source": [
        "Set Directory Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_8AjByntxVe",
        "outputId": "a60504cc-ae8d-4232-8fa3-b4cd5d226521"
      },
      "outputs": [],
      "source": [
        "CSV_FILE = \"/content/HAM10000_metadata.csv\"\n",
        "IMG_DIR = \"/content/HAM10000_images\"\n",
        "OUT_DIR = \"/content/outputs\"\n",
        "\n",
        "import os\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"âœ… Paths ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umc0vDRbtxPv"
      },
      "outputs": [],
      "source": [
        "import torch, timm, numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "evVM9LhwtxHq",
        "outputId": "1ccddb51-8ecb-4af0-af91-f3ea9c99290f"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(CSV_FILE)\n",
        "print(\"âœ… Metadata shape:\", df.shape)\n",
        "print(\"âœ… Sample columns:\", df.columns.tolist())\n",
        "\n",
        "import os\n",
        "print(\"âœ… Total images:\", len(os.listdir(IMG_DIR)))\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIzJllMtwXh0"
      },
      "source": [
        "# Dataset and Preprocessing\n",
        "The dataset consists of labeled dermatoscopic images organized into class-specific directories. Images are resized and normalized to match the input requirements of the Vision Transformer.\n",
        "\n",
        "*Image Parameters*\n",
        "\n",
        "-Image size: 224 Ã— 224\n",
        "\n",
        "-Color channels: 3 (RGB)\n",
        "\n",
        "*Data Augmentation (Training Only)*\n",
        "\n",
        "-Random resized cropping\n",
        "\n",
        "-Horizontal flipping\n",
        "\n",
        "-Vertical flipping\n",
        "\n",
        "-Brightness and contrast adjustment\n",
        "\n",
        "-Normalization\n",
        "\n",
        "Validation and test images are only resized and normalized to ensure unbiased evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "23e08206cc974ba1b5664dfd8eaf9b14",
            "a6a98873ed1a455d877ac27592972d59",
            "d5ca527495b546d087e6a9d3058dec73",
            "34a866691b0d4c668208a51df2e10b5b",
            "2455b88fbd0c481580ffbd87d6cd6660",
            "eeda8cc433b24e68b65f93488ed33da8",
            "bea98111e30e42bfb9b0deeb33d2b79d",
            "953c7549efa943b580e92989aef976a9",
            "5f794adef5e24fcb8ed8ae13a4876630",
            "61179bd67a724382879267abf704e3e1",
            "d64297043e2747c7b78e89e512351855"
          ]
        },
        "id": "aJy3y0dduIMx",
        "outputId": "05696057-035e-4d2c-b304-9adb5d037a98"
      },
      "outputs": [],
      "source": [
        "# ==== CONFIG ====\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LR = 3e-5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ==== PREPARE DATA ====\n",
        "df['label_str'] = df['dx']\n",
        "classes = sorted(df['label_str'].unique())\n",
        "df['label'] = df['label_str'].map({c:i for i,c in enumerate(classes)})\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)\n",
        "\n",
        "#remove hair_removal\n",
        "def remove_hair(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    #black-hat filtering\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "    blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
        "\n",
        "    #threshold\n",
        "    _, thresh = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    #inpaint\n",
        "    image=cv2.inpaint(image,thresh,1,cv2.INPAINT_TELEA)\n",
        "    return image\n",
        "\n",
        "def get_transforms(phase):\n",
        "    if phase == 'train':\n",
        "        return A.Compose([\n",
        "            A.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.8, 1.0)),\n",
        "            A.HorizontalFlip(),\n",
        "            A.VerticalFlip(p=0.2),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "\n",
        "            A.Normalize(),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
        "            A.Normalize(),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "\n",
        "class SkinDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = np.array(Image.open(os.path.join(self.img_dir, row['image_id'] + '.jpg')).convert('RGB'))\n",
        "        if self.transform: img = self.transform(image=img)['image']\n",
        "        label = int(row['label'])\n",
        "        return img, label\n",
        "\n",
        "train_loader = DataLoader(SkinDataset(train_df, IMG_DIR, get_transforms('train')), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(SkinDataset(val_df, IMG_DIR, get_transforms('val')), batch_size=BATCH_SIZE)\n",
        "\n",
        "# ==== MODEL ====\n",
        "# Adding drop_out rate to the model\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=len(classes),drop_rate=0.3,drop_path_rate=0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "\n",
        "# ==== TRAINING ====\n",
        "best_acc = 0\n",
        "train_losses, val_accuracies, train_accs = [], [], []  # added train_accs list\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # === Compute Training Accuracy ===\n",
        "    model.eval()\n",
        "    train_preds, train_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            train_true.extend(labels.cpu().numpy())\n",
        "    train_acc = accuracy_score(train_true, train_preds)\n",
        "\n",
        "    # === Validation ===\n",
        "    preds, true = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            true.extend(labels.cpu().numpy())\n",
        "    val_acc = accuracy_score(true, preds)\n",
        "\n",
        "    # === Log and Save ===\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(OUT_DIR, \"best_model.pth\"))\n",
        "        print(\"âœ… Saved best model\")\n",
        "\n",
        "\n",
        "# === Save Final Model ===\n",
        "torch.save(model.state_dict(), os.path.join(OUT_DIR, \"final_model.pth\"))\n",
        "print(\"âœ… Final model saved manually!\")\n",
        "print(\"Training Complete! Best Val Accuracy:\", best_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE5eD0xA0OUo"
      },
      "source": [
        "Visualize some mis-classified images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "xxbqkMIn7IW0",
        "outputId": "e7e78f37-d329-4a3f-9301-cdbcc5f4c45f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==== LOAD BEST MODEL ====\n",
        "model.load_state_dict(torch.load(os.path.join(OUT_DIR, \"best_model.pth\"), map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# ==== PREDICT ON VALIDATION SET ====\n",
        "misclassified = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(1)\n",
        "        for i in range(len(preds)):\n",
        "            if preds[i] != labels[i]:\n",
        "                misclassified.append((images[i].cpu(), preds[i].cpu().item(), labels[i].cpu().item()))\n",
        "\n",
        "# ==== SHOW SOME MISCLASSIFIED IMAGES ====\n",
        "def imshow(img, title):\n",
        "    img = img.permute(1, 2, 0).numpy()\n",
        "    img = (img - img.min()) / (img.max() - img.min())  # normalize for display\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Display first 6 misclassified samples\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, (img, pred, true) in enumerate(misclassified[:6]):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    imshow(img, f\"Pred: {classes[pred]}\\nTrue: {classes[true]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "a0m04qX67NQK",
        "outputId": "fa2cdfd5-6ed7-4daf-d2f7-f34e6faadb1b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1, EPOCHS+1), train_losses, marker='o')\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1, EPOCHS+1), val_accuracies, marker='o', color='orange')\n",
        "plt.title(\"Validation Accuracy per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdQIWkUQyBza"
      },
      "source": [
        "# Implement Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "daLO-ju97RcH",
        "outputId": "071990e9-a88d-4bc1-d771-358c577b1ab9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(xticks_rotation=45, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZgClD5UyGqZ"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "aGCFgYaPZZKu",
        "outputId": "ffdbd05c-2f0b-4716-9710-11bd46f468b9"
      },
      "outputs": [],
      "source": [
        "# === Validation Curve (Training vs Validation Accuracy) ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(val_accuracies) + 1)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(epochs, train_accs, label='Train Accuracy', color='royalblue')\n",
        "plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='darkorange')\n",
        "\n",
        "plt.title('Training vs Validation Accuracy ViT Model', fontsize=13, weight='bold')\n",
        "plt.xlabel('Epoch', fontsize=11)\n",
        "plt.ylabel('Accuracy', fontsize=11)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM1clE9hZeTI",
        "outputId": "faa93050-c24c-4140-eee6-861e33dfb751"
      },
      "outputs": [],
      "source": [
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=len(classes)).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/outputs/best_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded successfully for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w9BSXvpeZoT5",
        "outputId": "c9aa8bb3-51fa-490d-faba-a1d10f672f14"
      },
      "outputs": [],
      "source": [
        "# ==== EVALUATION FOR TRAIN, VAL, AND TEST SETS ====\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === Load the best model ===\n",
        "best_model_path = os.path.join(OUT_DIR, \"best_model.pth\")\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(f\"âœ… Loaded best model from {best_model_path}\")\n",
        "else:\n",
        "    print(\"âš ï¸ Best model not found, using current weights\")\n",
        "\n",
        "# === Helper Function for Confusion Matrix & Report ===\n",
        "def evaluate_model(loader, split_name=\"Validation\"):\n",
        "    y_true, y_pred = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    # === Accuracy ===\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\nðŸ“Š {split_name} Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # === Confusion Matrix ===\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(f\"{split_name} Confusion Matrix ViT Model\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # === Normalized Confusion Matrix ===\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm_norm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Greens\",\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(f\"{split_name} Normalized Confusion Matrix ViT Model\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # === Classification Report ===\n",
        "    print(f\"\\nðŸ§¾ {split_name} Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=classes))\n",
        "\n",
        "# === Evaluate on all splits ===\n",
        "evaluate_model(train_loader, \"Training\")\n",
        "evaluate_model(val_loader, \"Validation\")\n",
        "evaluate_model(DataLoader(SkinDataset(test_df, IMG_DIR, get_transforms('val')), batch_size=BATCH_SIZE), \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd1Bmt-Xqu39",
        "outputId": "97af2625-ee7a-4325-ba8e-c576205c09dd"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# 1) Find the first Conv2d layer\n",
        "first_conv_name, first_conv = None, None\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        first_conv_name, first_conv = name, module\n",
        "        break\n",
        "\n",
        "print(\"First Conv layer:\", first_conv_name, first_conv)\n",
        "\n",
        "# 2) Register a forward hook to capture its output\n",
        "feature_maps = {}\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    feature_maps['first_conv'] = output.detach().cpu()\n",
        "\n",
        "hook_handle = first_conv.register_forward_hook(hook_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbONsa-krexH",
        "outputId": "e6ed3deb-32d9-4101-99b4-b40bf0b5b0be"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD6dlo7xyUsN"
      },
      "source": [
        "# Visualizing model architecture details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mm-DwrPrhbF",
        "outputId": "6dca6ffb-b9a9-443f-bc80-acf6965861f9"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# For ViT base, input is 3Ã—224Ã—224; batch size 1\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(1, 3, 224, 224),\n",
        "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"),\n",
        "    depth=4,   # increase if you want to expand deeper nested blocks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUcHA_TTya5o"
      },
      "source": [
        "# Visualize simplified ViT architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "foTsRrwIumpv",
        "outputId": "e9d8462e-4297-42eb-acb4-706f60a8f397"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "\n",
        "# ===== Helper: draw a fake 3D block =====\n",
        "def draw_block(ax, x, y, w, h, d, label, face_color=\"#DDDDDD\"):\n",
        "    \"\"\"\n",
        "    x, y : front-bottom-left corner\n",
        "    w, h : width, height of front face\n",
        "    d    : depth offset\n",
        "    \"\"\"\n",
        "    # Front rectangle\n",
        "    front = [(x, y), (x+w, y), (x+w, y+h), (x, y+h)]\n",
        "    # Top rectangle\n",
        "    top = [(x, y+h), (x+d, y+h+d), (x+w+d, y+h+d), (x+w, y+h)]\n",
        "    # Side rectangle\n",
        "    side = [(x+w, y), (x+w+d, y+d), (x+w+d, y+h+d), (x+w, y+h)]\n",
        "\n",
        "    for poly_pts in [front, top, side]:\n",
        "        poly = Polygon(poly_pts, closed=True,\n",
        "                       facecolor=face_color, edgecolor=\"black\")\n",
        "        ax.add_patch(poly)\n",
        "\n",
        "    # Put text on the front face\n",
        "    ax.text(\n",
        "        x + w/2,\n",
        "        y + h/2,\n",
        "        label,\n",
        "        ha=\"center\", va=\"center\",\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "# ===== Create the diagram =====\n",
        "fig, ax = plt.subplots(figsize=(14, 4))\n",
        "\n",
        "# Coordinates & sizes\n",
        "y = 0\n",
        "h = 2.8\n",
        "w = 2.4\n",
        "d = 0.5\n",
        "gap = 1.2\n",
        "\n",
        "x0 = 0\n",
        "x1 = x0 + w + gap\n",
        "x2 = x1 + w + gap\n",
        "x3 = x2 + w + gap\n",
        "x4 = x3 + w + gap\n",
        "\n",
        "# Block 1: Input\n",
        "label_input = \"Input\\n224Ã—224Ã—3\"\n",
        "draw_block(ax, x0, y, w, h, d, label_input, face_color=\"#F2F2F2\")\n",
        "\n",
        "# Block 2: Patch Embedding\n",
        "label_patch = (\n",
        "    \"Patch Embedding\\n\"\n",
        "    \"16Ã—16 Conv\\n\"\n",
        "    \"196 tokens Ã— 768\"\n",
        ")\n",
        "draw_block(ax, x1, y, w, h, d, label_patch, face_color=\"#D0E4F7\")\n",
        "\n",
        "# Block 3: Transformer Encoder\n",
        "label_encoder = (\n",
        "    \"Transformer Encoder\\n\"\n",
        "    \"12 blocks\\n\"\n",
        "    \"MHSA + MLP\"\n",
        ")\n",
        "draw_block(ax, x2, y, w, h, d, label_encoder, face_color=\"#D9EAD3\")\n",
        "\n",
        "# Block 4: MLP Head\n",
        "label_head = (\n",
        "    \"MLP Head\\n\"\n",
        "    \"768 â†’ C\"\n",
        ")\n",
        "draw_block(ax, x3, y, w, h, d, label_head, face_color=\"#FCE5CD\")\n",
        "\n",
        "# Block 5: Output\n",
        "label_output = \"Output\\nPredicted Class\"\n",
        "draw_block(ax, x4, y, w, h, d, label_output, face_color=\"#F4CCCC\")\n",
        "\n",
        "# Arrows between blocks\n",
        "def arrow_between(x_start, x_end, y_center):\n",
        "    ax.annotate(\n",
        "        \"\",\n",
        "        xy=(x_end - 0.2, y_center),\n",
        "        xytext=(x_start + w + 0.2, y_center),\n",
        "        arrowprops=dict(arrowstyle=\"->\", linewidth=1.5)\n",
        "    )\n",
        "\n",
        "y_center = y + h/2\n",
        "arrow_between(x0, x1, y_center)\n",
        "arrow_between(x1, x2, y_center)\n",
        "arrow_between(x2, x3, y_center)\n",
        "arrow_between(x3, x4, y_center)\n",
        "\n",
        "# Title\n",
        "ax.text(\n",
        "    (x0 + x4 + w) / 2, y + h + 1.0,\n",
        "    \"Simplified Vision Transformer Architecture\",\n",
        "    ha=\"center\", va=\"center\",\n",
        "    fontsize=14, fontweight=\"bold\"\n",
        ")\n",
        "\n",
        "ax.set_xlim(-1, x4 + w + 1)\n",
        "ax.set_ylim(-1, y + h + 2)\n",
        "ax.axis(\"off\")\n",
        "\n",
        "# No tight_layout to avoid margin warnings\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23e08206cc974ba1b5664dfd8eaf9b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6a98873ed1a455d877ac27592972d59",
              "IPY_MODEL_d5ca527495b546d087e6a9d3058dec73",
              "IPY_MODEL_34a866691b0d4c668208a51df2e10b5b"
            ],
            "layout": "IPY_MODEL_2455b88fbd0c481580ffbd87d6cd6660"
          }
        },
        "2455b88fbd0c481580ffbd87d6cd6660": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a866691b0d4c668208a51df2e10b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61179bd67a724382879267abf704e3e1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d64297043e2747c7b78e89e512351855",
            "value": "â€‡346M/346Mâ€‡[00:07&lt;00:00,â€‡90.6MB/s]"
          }
        },
        "5f794adef5e24fcb8ed8ae13a4876630": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61179bd67a724382879267abf704e3e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953c7549efa943b580e92989aef976a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6a98873ed1a455d877ac27592972d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeda8cc433b24e68b65f93488ed33da8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bea98111e30e42bfb9b0deeb33d2b79d",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "bea98111e30e42bfb9b0deeb33d2b79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5ca527495b546d087e6a9d3058dec73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953c7549efa943b580e92989aef976a9",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f794adef5e24fcb8ed8ae13a4876630",
            "value": 346284714
          }
        },
        "d64297043e2747c7b78e89e512351855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeda8cc433b24e68b65f93488ed33da8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
